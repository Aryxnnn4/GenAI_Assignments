{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d226eedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Activation Functions Assignment (Modified Version)\n",
    "# Author: Aryan\n",
    "\n",
    "# 1. Sigmoid Activation\n",
    "# Formula: σ(x) = 1 / (1 + exp(-x))\n",
    "def sigmoid_fn(values):\n",
    "    vals = np.array(values, dtype=float)\n",
    "    return 1 / (1 + np.exp(-vals))\n",
    "\n",
    "sig_inputs = [-7.5, -2.5, -0.8, 0.2, 2.2, 6.5]\n",
    "sig_outputs = sigmoid_fn(sig_inputs)\n",
    "print(\"Sigmoid Inputs :\", sig_inputs)\n",
    "print(\"Sigmoid Outputs:\", sig_outputs)\n",
    "\n",
    "# 2. Tanh Activation\n",
    "# Formula: tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))\n",
    "def tanh_fn(values):\n",
    "    return np.tanh(np.array(values, dtype=float))\n",
    "\n",
    "tanh_inputs = [-7.5, -2.5, -0.8, 0.2, 2.2, 6.5]\n",
    "tanh_outputs = tanh_fn(tanh_inputs)\n",
    "print(\"\\nTanh Inputs :\", tanh_inputs)\n",
    "print(\"Tanh Outputs:\", tanh_outputs)\n",
    "\n",
    "# 3. ReLU Activation\n",
    "# Formula: ReLU(x) = max(0, x)\n",
    "def relu_fn(values):\n",
    "    vals = np.array(values, dtype=float)\n",
    "    return np.where(vals > 0, vals, 0)\n",
    "\n",
    "relu_inputs = [-7.5, -2.5, -0.8, 0.2, 2.2, 6.5]\n",
    "relu_outputs = relu_fn(relu_inputs)\n",
    "print(\"\\nReLU Inputs :\", relu_inputs)\n",
    "print(\"ReLU Outputs:\", relu_outputs)\n",
    "\n",
    "# 4. Leaky ReLU Activation\n",
    "# Formula: LReLU(x) = x if x > 0 else αx\n",
    "def leaky_relu_fn(values, alpha=0.05):\n",
    "    vals = np.array(values, dtype=float)\n",
    "    return np.where(vals >= 0, vals, alpha * vals)\n",
    "\n",
    "leaky_inputs = [-7.5, -2.5, -0.8, 0.2, 2.2, 6.5]\n",
    "leaky_outputs = leaky_relu_fn(leaky_inputs)\n",
    "print(\"\\nLeaky ReLU Inputs :\", leaky_inputs)\n",
    "print(\"Leaky ReLU Outputs:\", leaky_outputs)\n",
    "\n",
    "# 5. ELU Activation\n",
    "# Formula: ELU(x) = x if x >= 0 else α(exp(x) - 1)\n",
    "def elu_fn(values, alpha=1.2):\n",
    "    vals = np.array(values, dtype=float)\n",
    "    return np.where(vals >= 0, vals, alpha * (np.exp(vals) - 1))\n",
    "\n",
    "elu_inputs = [-7.5, -2.5, -0.8, 0.2, 2.2, 6.5]\n",
    "elu_outputs = elu_fn(elu_inputs)\n",
    "print(\"\\nELU Inputs :\", elu_inputs)\n",
    "print(\"ELU Outputs:\", elu_outputs)\n",
    "\n",
    "# 6. Softmax Activation\n",
    "# Formula: softmax(x_i) = exp(x_i) / Σ exp(x_j)\n",
    "def softmax_fn(values):\n",
    "    vals = np.array(values, dtype=float)\n",
    "    shifted = vals - np.max(vals)\n",
    "    exp_vals = np.exp(shifted)\n",
    "    return exp_vals / np.sum(exp_vals)\n",
    "\n",
    "soft_inputs = [-7.5, -2.5, -0.8, 0.2, 2.2, 6.5]\n",
    "soft_outputs = softmax_fn(soft_inputs)\n",
    "print(\"\\nSoftmax Inputs :\", soft_inputs)\n",
    "print(\"Softmax Outputs:\", soft_outputs)\n",
    "\n",
    "# Combined Evaluation\n",
    "test_data = [[-7.5, -2.5, -0.8, 0.2, 2.2, 6.5]]\n",
    "print(\"\\n================ Combined Output ================\")\n",
    "print(\"Input Data:\", test_data)\n",
    "\n",
    "print(\"\\nSigmoid:\")\n",
    "print(sigmoid_fn(test_data))\n",
    "\n",
    "print(\"\\nTanh:\")\n",
    "print(tanh_fn(test_data))\n",
    "\n",
    "print(\"\\nReLU:\")\n",
    "print(relu_fn(test_data))\n",
    "\n",
    "print(\"\\nLeaky ReLU:\")\n",
    "print(leaky_relu_fn(test_data))\n",
    "\n",
    "print(\"\\nELU:\")\n",
    "print(elu_fn(test_data))\n",
    "\n",
    "print(\"\\nSoftmax:\")\n",
    "print(softmax_fn(test_data))\n",
    "\n",
    "print(\"================================================\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
